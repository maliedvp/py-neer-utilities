{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"A Minimal Training Pipeline\"\n",
        "---"
      ],
      "id": "9eb669e0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This document explains the data preparation process for training our\n",
        "matching model. The example data comes from a research project that\n",
        "digitized historic records of German joint-stock companies [(Gram et\n",
        "al.Â 2022)](https://dl.acm.org/doi/10.1145/3531533). The data contains\n",
        "inconsistencies in spelling, primarily due to variations in abbreviation\n",
        "conventions and OCR errors, across most variables. These challenges make\n",
        "it a compelling real-world use case for entity matching.\n",
        "\n",
        "The data consists of three files:\n",
        "\n",
        "- *left.csv*\n",
        "- *right.csv*\n",
        "- *matches.csv*\n",
        "\n",
        "## Loading the Data\n",
        "\n",
        "Training the pipelines requires three datasets:\n",
        "\n",
        "- `left` (observations from one source or period)\n",
        "- `right` (observations from another source or period)\n",
        "- `matches` (a dataframe where each row contains the unique IDs of matching entities from `left` and `right`)\n"
      ],
      "id": "83ad8ef2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "matches = pd.read_csv('matches.csv')\n",
        "left = pd.read_csv('left.csv')\n",
        "right = pd.read_csv('right.csv')"
      ],
      "id": "d0473346",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preview of the matches data:\n"
      ],
      "id": "cf6f9a16"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "matches.head()"
      ],
      "id": "fce1da7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preview of the left dataset:\n"
      ],
      "id": "cac655cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "left.head()"
      ],
      "id": "99150a98",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preview of the right dataset:\n"
      ],
      "id": "f63514c8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "right.head()"
      ],
      "id": "92f763d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining Features and Similarity Concepts\n",
        "\n",
        "The `similarity_map` defines which similarity concepts (values) to apply to each feature pair (keys). Note that this example uses a minimal similarity map for simplicity rather than optimal performance.\n"
      ],
      "id": "95efc515"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from neer_match.similarity_map import SimilarityMap\n",
        "from neer_match_utilities.custom_similarities import CustomSimilarities\n",
        "\n",
        "CustomSimilarities() # Ensures Similarity concepts are always scaled between 0 and 1.\n",
        "\n",
        "# Define similarity_map\n",
        "\n",
        "similarity_map = {\n",
        "    \"company_name\" : [\n",
        "        \"levenshtein\",\n",
        "        \"jaro_winkler\",\n",
        "        \"partial_token_sort_ratio\",\n",
        "    ],\n",
        "    \"city\" : [\n",
        "        \"levenshtein\",\n",
        "    ],\n",
        "    \"industry\" : [\n",
        "        \"levenshtein\",\n",
        "        \"jaro_winkler\",\n",
        "        \"notmissing\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "smap = SimilarityMap(similarity_map)"
      ],
      "id": "10dbb706",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Harmonizing the data\n",
        "\n",
        "### Left and Right\n",
        "\n",
        "Next, data formatting can be harmonized using the `Prepare` class. This class offers flexible arguments for operations such as capitalizing strings, converting values to numeric types, and filling missing values. Additionally, a spaCy pipeline and custom stop words can be specified to remove noise from string variables (see [additional functionalities](additional_functionalities.md)). All operations are applied consistently to both the *left* and *right* DataFrames.\n"
      ],
      "id": "80e6e31c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from neer_match_utilities.prepare import Prepare\n",
        "\n",
        "# Initialize the Prepare object\n",
        "\n",
        "prepare = Prepare(\n",
        "    similarity_map=similarity_map, \n",
        "    df_left=left, \n",
        "    df_right=right, \n",
        "    id_left='company_id', \n",
        "    id_right='company_id',\n",
        ")\n",
        "\n",
        "# Get formatted and harmonized datasets\n",
        "\n",
        "left, right = prepare.format(\n",
        "    fill_numeric_na=False,\n",
        "    to_numeric=['found_year'],\n",
        "    fill_string_na=True, \n",
        "    capitalize=True,\n",
        "    lower_case=False,\n",
        ")"
      ],
      "id": "2cdf86e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "left.head()"
      ],
      "id": "8804f7a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Re-Structuring the `Matches` dataframe\n",
        "\n",
        "`neer-match` requires that the *matches* DataFrame be structured with\n",
        "the indices from the left and right datasets instead of their unique\n",
        "IDs. To convert your *matches* DataFrame into the required format, you\n",
        "can run:\n"
      ],
      "id": "a6e8ca23"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from neer_match_utilities.training import Training\n",
        "\n",
        "training = Training(\n",
        "    similarity_map=similarity_map, \n",
        "    df_left=left, \n",
        "    df_right=right, \n",
        "    id_left='company_id', \n",
        "    id_right='company_id',\n",
        ")\n",
        "\n",
        "matches = training.matches_reorder(\n",
        "    matches, \n",
        "    matches_id_left='company_id_left', \n",
        "    matches_id_right='company_id_right'\n",
        ")\n",
        "\n",
        "matches.head()"
      ],
      "id": "f9605674",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Splitting Data\n",
        "\n",
        "Subsequently, we need to split the data into training and test sets,\n",
        "each consisting of three DataFrames. The training ratio is given by\n",
        "$\\text{training_ratio} = 1 - (\\text{test_ratio} + \\text{validation_ratio})$.\n",
        "Note that since validation is not implemented yet, you can set\n",
        "$\\text{validation_ratio} = 0$.\n"
      ],
      "id": "6bea2bf0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from neer_match_utilities.split import split_test_train\n",
        "\n",
        "left_train, right_train, matches_train, left_validation, right_validation, matches_validation, left_test, right_test, matches_test = split_test_train(\n",
        "    left = left,\n",
        "    right = right,\n",
        "    matches = matches,\n",
        "    test_ratio = .5,\n",
        "    validation_ratio = .0\n",
        ")"
      ],
      "id": "8cdad723",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and Exporting the Model\n",
        "\n",
        "For this tutorial, we use a simple Logit model. Other models (ANN, Probit, or GradientBoost) follow a similar syntax and are covered in [alternative models](alternative_models.md).\n"
      ],
      "id": "4c3a1296"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from neer_match_utilities.baseline_training import BaselineTrainingPipe\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "training_pipeline = BaselineTrainingPipe(\n",
        "    model_name='demonstration_model',\n",
        "    similarity_map=smap,\n",
        "    training_data=(left_train, right_train, matches_train),\n",
        "    validation_data=(left_validation, right_validation, matches_validation),  # only needed if tune_threshold for GB\n",
        "    testing_data=(left_test, right_test, matches_test),\n",
        "    id_left_col=\"company_id\",\n",
        "    id_right_col=\"company_id\",\n",
        "    # matches_id_left=\"left\",\n",
        "    # matches_id_right=\"right\",\n",
        "    model_kind=\"logit\", # \"logit\" | \"probit\" | \"gb\"\n",
        "    mismatch_share_fit=1.0,\n",
        "    # tune_threshold=False, # recommended for \"gb\"\n",
        "    # tune_metric=\"mcc\",\n",
        ")\n",
        "\n",
        "training_pipeline.execute()"
      ],
      "id": "64a2aa36",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}