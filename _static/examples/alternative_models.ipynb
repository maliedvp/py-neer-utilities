{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Alternative Classification Models\"\n",
        "execute:\n",
        "  eval: false\n",
        "---"
      ],
      "id": "9a51c691"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This document covers all classification models available in `neer_match_utilities` for entity matching. The examples assume you have already prepared your data as described in [basic training pipeline](basic_training_pipeline.md).\n",
        "\n",
        "## Baseline Models\n",
        "\n",
        "Baseline models (Logit, Probit, GradientBoost) use the `BaselineTrainingPipe` class. They are faster to train and serve as good benchmarks.\n",
        "\n",
        "### Logit Model\n",
        "\n",
        "Logistic regression using statsmodels. A simple, interpretable baseline.\n"
      ],
      "id": "fd4454d2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from neer_match_utilities.baseline_training import BaselineTrainingPipe\n",
        "\n",
        "training_pipeline = BaselineTrainingPipe(\n",
        "    # Required\n",
        "    model_name='my_logit_model',\n",
        "    similarity_map=smap,\n",
        "    training_data=(left_train, right_train, matches_train),\n",
        "    testing_data=(left_test, right_test, matches_test),\n",
        "\n",
        "    # Model type\n",
        "    model_kind=\"logit\",  # \"logit\" | \"probit\" | \"gb\"\n",
        "\n",
        "    # ID columns (must match your data)\n",
        "    id_left_col=\"company_id\",\n",
        "    id_right_col=\"company_id\",\n",
        "\n",
        "    # How matches dataframe is structured\n",
        "    matches_id_left=\"left\",       # column name for left IDs in matches\n",
        "    matches_id_right=\"right\",     # column name for right IDs in matches\n",
        "    matches_are_indices=True,     # True if matches contain row indices, False if IDs\n",
        "\n",
        "    # Sampling: fraction of non-matches to use during fitting\n",
        "    mismatch_share_fit=1.0,       # 1.0 = use all, 0.1 = use 10%\n",
        "    random_state=42,\n",
        "    shuffle_fit=True,\n",
        "\n",
        "    # Prediction threshold\n",
        "    threshold=0.5,\n",
        "    tune_threshold=False,         # Logit/Probit: typically use 0.5\n",
        "\n",
        "    # Optional: validation data for threshold tuning\n",
        "    validation_data=None,         # (left_val, right_val, matches_val)\n",
        "\n",
        "    # Export settings\n",
        "    base_dir=None,                # defaults to current directory\n",
        "    export_model=True,\n",
        "    export_stats=True,\n",
        "    reload_sanity_check=True,     # verify model can be reloaded\n",
        ")\n",
        "\n",
        "training_pipeline.execute()"
      ],
      "id": "398de25f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Probit Model\n",
        "\n",
        "Probit regression using statsmodels. Similar to Logit but uses the cumulative normal distribution.\n"
      ],
      "id": "d201025a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "training_pipeline = BaselineTrainingPipe(\n",
        "    model_name='my_probit_model',\n",
        "    similarity_map=smap,\n",
        "    training_data=(left_train, right_train, matches_train),\n",
        "    testing_data=(left_test, right_test, matches_test),\n",
        "\n",
        "    model_kind=\"probit\",\n",
        "\n",
        "    id_left_col=\"company_id\",\n",
        "    id_right_col=\"company_id\",\n",
        "    matches_id_left=\"left\",\n",
        "    matches_id_right=\"right\",\n",
        "    matches_are_indices=True,\n",
        "\n",
        "    mismatch_share_fit=1.0,\n",
        "    threshold=0.5,\n",
        "    tune_threshold=False,\n",
        "\n",
        "    export_model=True,\n",
        "    export_stats=True,\n",
        ")\n",
        "\n",
        "training_pipeline.execute()"
      ],
      "id": "4b0ba339",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gradient Boosting Model\n",
        "\n",
        "Gradient Boosting using scikit-learn. More powerful but less interpretable.\n"
      ],
      "id": "58e11966"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "training_pipeline = BaselineTrainingPipe(\n",
        "    model_name='my_gb_model',\n",
        "    similarity_map=smap,\n",
        "    training_data=(left_train, right_train, matches_train),\n",
        "    testing_data=(left_test, right_test, matches_test),\n",
        "\n",
        "    model_kind=\"gb\",\n",
        "\n",
        "    id_left_col=\"company_id\",\n",
        "    id_right_col=\"company_id\",\n",
        "    matches_id_left=\"left\",\n",
        "    matches_id_right=\"right\",\n",
        "    matches_are_indices=True,\n",
        "\n",
        "    # Sampling (GB often works well with subsampling)\n",
        "    mismatch_share_fit=0.5,       # use 50% of non-matches\n",
        "\n",
        "    # Threshold tuning (recommended for GB)\n",
        "    tune_threshold=True,          # automatically find best threshold\n",
        "    tune_metric=\"mcc\",            # \"mcc\" or \"f1\"\n",
        "    validation_data=(left_val, right_val, matches_val),  # required for tuning\n",
        "\n",
        "    export_model=True,\n",
        "    export_stats=True,\n",
        ")\n",
        "\n",
        "training_pipeline.execute()"
      ],
      "id": "feaaa06c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep Learning Model (ANN)\n",
        "\n",
        "The neural network model uses `TrainingPipe` and supports two-stage training with customizable loss functions.\n"
      ],
      "id": "c0ab72d9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from neer_match_utilities.training import TrainingPipe\n",
        "\n",
        "training_pipeline = TrainingPipe(\n",
        "    # Required\n",
        "    model_name='my_ann_model',\n",
        "    similarity_map=similarity_map,  # dict format, not SimilarityMap object\n",
        "    training_data=(left_train, right_train, matches_train),\n",
        "    testing_data=(left_test, right_test, matches_test),\n",
        "\n",
        "    # ID columns\n",
        "    id_left_col=\"company_id\",\n",
        "    id_right_col=\"company_id\",\n",
        "\n",
        "    # Network architecture\n",
        "    initial_feature_width_scales=10,  # width multiplier for feature networks\n",
        "    feature_depths=2,                  # depth of feature networks\n",
        "    initial_record_width_scale=10,     # width multiplier for record network\n",
        "    record_depth=4,                    # depth of record network\n",
        "\n",
        "    # Stage 1: Soft-F1 pretraining\n",
        "    stage_1=True,\n",
        "    epochs_1=50,\n",
        "    mismatch_share_1=0.01,            # fraction of non-matches per epoch\n",
        "    stage1_loss=\"soft_f1\",            # \"soft_f1\" | \"binary_crossentropy\" | callable\n",
        "\n",
        "    # Stage 2: Focal loss fine-tuning\n",
        "    stage_2=True,\n",
        "    epochs_2=30,\n",
        "    mismatch_share_2=0.1,\n",
        "    gamma=2.0,                        # focal loss focusing parameter\n",
        "    max_alpha=0.9,                    # max weight for positive class\n",
        "\n",
        "    # Batch size control\n",
        "    no_tm_pbatch=8,                   # target positives per batch\n",
        "\n",
        "    # Export\n",
        "    save_architecture=False,          # requires graphviz binaries\n",
        ")\n",
        "\n",
        "training_pipeline.execute()"
      ],
      "id": "526391a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Parameters Explained\n",
        "\n",
        "**Network Architecture:**\n",
        "\n",
        "- `initial_feature_width_scales`: Controls the width of feature-specific networks. Higher values create wider networks.\n",
        "- `feature_depths`: Number of layers in each feature network.\n",
        "- `initial_record_width_scale`: Controls the width of the final record-comparison network.\n",
        "- `record_depth`: Number of layers in the record network.\n",
        "\n",
        "**Training Stages:**\n",
        "\n",
        "- `stage_1`: Pretraining phase using soft-F1 loss to learn basic matching patterns.\n",
        "- `stage_2`: Fine-tuning phase using focal loss to focus on hard examples.\n",
        "- You can disable either stage by setting it to `False`.\n",
        "\n",
        "**Sampling:**\n",
        "\n",
        "- `mismatch_share_1/2`: Fraction of non-matches to sample per epoch. Lower values speed up training but may reduce quality.\n",
        "- `no_tm_pbatch`: Target number of positive pairs per batch. The actual batch size is calculated automatically.\n",
        "\n",
        "**Focal Loss (Stage 2):**\n",
        "\n",
        "- `gamma`: Focusing parameter. Higher values focus more on hard examples (typical: 1.0-3.0).\n",
        "- `max_alpha`: Maximum class weight for positives. Balances class imbalance.\n",
        "\n",
        "### Single-Stage Training\n",
        "\n",
        "You can run only one training stage:\n"
      ],
      "id": "cd808e09"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Only Stage 1 (faster, simpler)\n",
        "training_pipeline = TrainingPipe(\n",
        "    model_name='my_model_stage1_only',\n",
        "    similarity_map=similarity_map,\n",
        "    training_data=(left_train, right_train, matches_train),\n",
        "    testing_data=(left_test, right_test, matches_test),\n",
        "    id_left_col=\"company_id\",\n",
        "    id_right_col=\"company_id\",\n",
        "\n",
        "    stage_1=True,\n",
        "    epochs_1=100,\n",
        "    mismatch_share_1=0.05,\n",
        "    no_tm_pbatch=8,\n",
        "\n",
        "    stage_2=False,  # Skip stage 2\n",
        ")\n",
        "\n",
        "training_pipeline.execute()"
      ],
      "id": "4a886bf2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Comparison\n",
        "\n",
        "| Model | Linear | Speed | Interpretability |\n",
        "|-------|--------|-------|-----------------|\n",
        "| Logit | Yes | Fast | High |\n",
        "| Probit | Yes | Fast | High |\n",
        "| GB | No | Medium | Low |\n",
        "| ANN | No | Slow | Low |\n",
        "\n",
        "**Note on performance:** Model performance depends heavily on the specific use case, dataset characteristics, and hyperparameter tuning. ANNs are more prone to getting stuck in local minima, making their results more volatile across runs. Always compare against baseline models for your specific use case."
      ],
      "id": "d238643e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}