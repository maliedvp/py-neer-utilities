

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Training &mdash; Neer Match Utilities 1.0.26-beta documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/css/extra.css?v=503a7970" />
      <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/2.1.8/css/dataTables.dataTables.min.css" />

  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=703f0c8d"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
      <script src="https://cdn.datatables.net/2.1.8/js/dataTables.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="License" href="LICENSE.html" />
    <link rel="prev" title="Split" href="split.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Neer Match Utilities
              <img src="_static/logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="install.html#pipy">PiPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#from-source">From Source</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="preparation.html">Data Preparation for Training</a><ul>
<li class="toctree-l2"><a class="reference internal" href="preparation.html#different-data-relationships">Different Data Relationships</a><ul>
<li class="toctree-l3"><a class="reference internal" href="preparation.html#loading-the-data">1. Loading the Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="preparation.html#inspecting-the-data">2. Inspecting the Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="preparation.html#simulating-a-many-to-many-relationship">3. Simulating a Many-to-Many Relationship</a></li>
<li class="toctree-l3"><a class="reference internal" href="preparation.html#understanding-the-matching-issue">4. Understanding the Matching Issue</a></li>
<li class="toctree-l3"><a class="reference internal" href="preparation.html#correcting-the-relationships">5. Correcting the Relationships</a></li>
<li class="toctree-l3"><a class="reference internal" href="preparation.html#verifying-the-adjustments">6. Verifying the Adjustments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="preparation.html#formatting">Formatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="preparation.html#a-customized-similarity-map">1. A customized <code class="docutils literal notranslate"><span class="pre">similarity_map</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="preparation.html#harmonizing-the-data">2. Harmonizing the data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="preparation.html#re-structuring">Re-Structuring</a></li>
<li class="toctree-l2"><a class="reference internal" href="preparation.html#splitting-data">Splitting data</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model_save.html">Training and Saving a Model</a><ul>
<li class="toctree-l2"><a class="reference internal" href="model_save.html#prepare-the-data">Prepare the Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_save.html#train-the-model">Train the Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_save.html#export-the-model">Export the Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_save.html#export-performance-statistics">Export Performance Statistics</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model_load.html">Loading a Model</a><ul>
<li class="toctree-l2"><a class="reference internal" href="model_load.html#load-data-model">Load Data &amp; Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_load.html#harmonize-format">Harmonize Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_load.html#make-suggestions">Make Suggestions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="id_generation.html">Creating a Common Identifier (ID)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="id_generation.html#between-two-sources">Between Two Sources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="id_generation.html#load-data-model">1. Load Data &amp; Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="id_generation.html#harmonize-format">2. Harmonize Format</a></li>
<li class="toctree-l3"><a class="reference internal" href="id_generation.html#generate-a-common-id">3. Generate a Common ID</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="id_generation.html#repeated-cross-sections-panel-id">Repeated Cross-Sections (Panel ID)</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="base.html">Base</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Model</a><ul>
<li class="toctree-l2"><a class="reference internal" href="model.html#neer_match_utilities.model.EpochEndSaver"><code class="docutils literal notranslate"><span class="pre">EpochEndSaver</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="model.html#neer_match_utilities.model.EpochEndSaver.__init__"><code class="docutils literal notranslate"><span class="pre">EpochEndSaver.__init__()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="model.html#neer_match_utilities.model.EpochEndSaver.on_epoch_end"><code class="docutils literal notranslate"><span class="pre">EpochEndSaver.on_epoch_end()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model.html#neer_match_utilities.model.Model"><code class="docutils literal notranslate"><span class="pre">Model</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="model.html#neer_match_utilities.model.Model.load"><code class="docutils literal notranslate"><span class="pre">Model.load()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="model.html#neer_match_utilities.model.Model.save"><code class="docutils literal notranslate"><span class="pre">Model.save()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="panel.html">Panel</a><ul>
<li class="toctree-l2"><a class="reference internal" href="panel.html#neer_match_utilities.panel.GenerateID"><code class="docutils literal notranslate"><span class="pre">GenerateID</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="panel.html#neer_match_utilities.panel.GenerateID.__init__"><code class="docutils literal notranslate"><span class="pre">GenerateID.__init__()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="panel.html#neer_match_utilities.panel.GenerateID.assign_ids"><code class="docutils literal notranslate"><span class="pre">GenerateID.assign_ids()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="panel.html#neer_match_utilities.panel.GenerateID.execute"><code class="docutils literal notranslate"><span class="pre">GenerateID.execute()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="panel.html#neer_match_utilities.panel.GenerateID.generate_suggestions"><code class="docutils literal notranslate"><span class="pre">GenerateID.generate_suggestions()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="panel.html#neer_match_utilities.panel.GenerateID.group_by_subgroups"><code class="docutils literal notranslate"><span class="pre">GenerateID.group_by_subgroups()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="panel.html#neer_match_utilities.panel.GenerateID.harmonize_ids"><code class="docutils literal notranslate"><span class="pre">GenerateID.harmonize_ids()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="panel.html#neer_match_utilities.panel.GenerateID.relations_left_right"><code class="docutils literal notranslate"><span class="pre">GenerateID.relations_left_right()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="panel.html#neer_match_utilities.panel.SetupData"><code class="docutils literal notranslate"><span class="pre">SetupData</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="panel.html#neer_match_utilities.panel.SetupData.matches"><code class="docutils literal notranslate"><span class="pre">SetupData.matches</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="panel.html#neer_match_utilities.panel.SetupData.__init__"><code class="docutils literal notranslate"><span class="pre">SetupData.__init__()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="panel.html#neer_match_utilities.panel.SetupData.adjust_overlap"><code class="docutils literal notranslate"><span class="pre">SetupData.adjust_overlap()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="panel.html#neer_match_utilities.panel.SetupData.create_connected_groups"><code class="docutils literal notranslate"><span class="pre">SetupData.create_connected_groups()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="panel.html#neer_match_utilities.panel.SetupData.data_preparation_panel"><code class="docutils literal notranslate"><span class="pre">SetupData.data_preparation_panel()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="panel.html#neer_match_utilities.panel.SetupData.drop_repetitions"><code class="docutils literal notranslate"><span class="pre">SetupData.drop_repetitions()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="panel.html#neer_match_utilities.panel.SetupData.panel_preparation"><code class="docutils literal notranslate"><span class="pre">SetupData.panel_preparation()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="prepare.html">Prepare</a><ul>
<li class="toctree-l2"><a class="reference internal" href="prepare.html#neer_match_utilities.prepare.Prepare"><code class="docutils literal notranslate"><span class="pre">Prepare</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="prepare.html#neer_match_utilities.prepare.Prepare.__init__"><code class="docutils literal notranslate"><span class="pre">Prepare.__init__()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="prepare.html#neer_match_utilities.prepare.Prepare.do_remove_stop_words"><code class="docutils literal notranslate"><span class="pre">Prepare.do_remove_stop_words()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="prepare.html#neer_match_utilities.prepare.Prepare.format"><code class="docutils literal notranslate"><span class="pre">Prepare.format()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="prepare.html#neer_match_utilities.prepare.similarity_map_to_dict"><code class="docutils literal notranslate"><span class="pre">similarity_map_to_dict()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="prepare.html#neer_match_utilities.prepare.synth_mismatches"><code class="docutils literal notranslate"><span class="pre">synth_mismatches()</span></code></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="split.html">Split</a><ul>
<li class="toctree-l2"><a class="reference internal" href="split.html#neer_match_utilities.split.SplitError"><code class="docutils literal notranslate"><span class="pre">SplitError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="split.html#neer_match_utilities.split.split_test_train"><code class="docutils literal notranslate"><span class="pre">split_test_train()</span></code></a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Training</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#neer_match_utilities.training.Training"><code class="docutils literal notranslate"><span class="pre">Training</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#neer_match_utilities.training.Training.evaluate_dataframe"><code class="docutils literal notranslate"><span class="pre">Training.evaluate_dataframe()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#neer_match_utilities.training.Training.matches_reorder"><code class="docutils literal notranslate"><span class="pre">Training.matches_reorder()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#neer_match_utilities.training.Training.performance_statistics_export"><code class="docutils literal notranslate"><span class="pre">Training.performance_statistics_export()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#neer_match_utilities.training.TrainingPipe"><code class="docutils literal notranslate"><span class="pre">TrainingPipe</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#neer_match_utilities.training.TrainingPipe.WarmupCosine"><code class="docutils literal notranslate"><span class="pre">TrainingPipe.WarmupCosine</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#neer_match_utilities.training.TrainingPipe.WarmupCosine.__init__"><code class="docutils literal notranslate"><span class="pre">TrainingPipe.WarmupCosine.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#neer_match_utilities.training.TrainingPipe.WarmupCosine.from_config"><code class="docutils literal notranslate"><span class="pre">TrainingPipe.WarmupCosine.from_config()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#neer_match_utilities.training.TrainingPipe.__init__"><code class="docutils literal notranslate"><span class="pre">TrainingPipe.__init__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#neer_match_utilities.training.alpha_balanced"><code class="docutils literal notranslate"><span class="pre">alpha_balanced()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#neer_match_utilities.training.combined_loss"><code class="docutils literal notranslate"><span class="pre">combined_loss()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#neer_match_utilities.training.focal_loss"><code class="docutils literal notranslate"><span class="pre">focal_loss()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#neer_match_utilities.training.soft_f1_loss"><code class="docutils literal notranslate"><span class="pre">soft_f1_loss()</span></code></a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">License</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="LICENSE.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Neer Match Utilities</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Training</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/training.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-neer_match_utilities.training">
<span id="training"></span><h1>Training<a class="headerlink" href="#module-neer_match_utilities.training" title="Link to this heading"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="neer_match_utilities.training.Training">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neer_match_utilities.training.</span></span><span class="sig-name descname"><span class="pre">Training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">similarity_map</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">df_left=Empty</span> <span class="pre">DataFrame</span> <span class="pre">Columns:</span> <span class="pre">[]</span> <span class="pre">Index:</span> <span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">df_right=Empty</span> <span class="pre">DataFrame</span> <span class="pre">Columns:</span> <span class="pre">[]</span> <span class="pre">Index:</span> <span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id_left='id'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id_right='id'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neer_match_utilities/training.html#Training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neer_match_utilities.training.Training" title="Link to this definition"></a></dt>
<dd><p>A class for managing and evaluating training processes, including
reordering matches, evaluating performance metrics, and exporting models.</p>
<section id="inherits">
<h2>Inherits:<a class="headerlink" href="#inherits" title="Link to this heading"></a></h2>
<p>SuperClass : Base class providing shared attributes and methods.</p>
<dl class="py method">
<dt class="sig sig-object py" id="neer_match_utilities.training.Training.evaluate_dataframe">
<span class="sig-name descname"><span class="pre">evaluate_dataframe</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">evaluation_test</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_train</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neer_match_utilities/training.html#Training.evaluate_dataframe"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neer_match_utilities.training.Training.evaluate_dataframe" title="Link to this definition"></a></dt>
<dd><p>Combines and evaluates test and training performance metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>evaluation_test</strong> (<em>dict</em>) – Dictionary containing performance metrics for the test dataset.</p></li>
<li><p><strong>evaluation_train</strong> (<em>dict</em>) – Dictionary containing performance metrics for the training dataset.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A DataFrame with accuracy, precision, recall, F-score, and a timestamp
for both test and training datasets.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pd.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neer_match_utilities.training.Training.matches_reorder">
<span class="sig-name descname"><span class="pre">matches_reorder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">matches</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">matches_id_left</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">matches_id_right</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neer_match_utilities/training.html#Training.matches_reorder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neer_match_utilities.training.Training.matches_reorder" title="Link to this definition"></a></dt>
<dd><p>Reorders a matches DataFrame to include indices from the left and
right DataFrames instead of their original IDs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>matches</strong> (<em>pd.DataFrame</em>) – DataFrame containing matching pairs.</p></li>
<li><p><strong>matches_id_left</strong> (<em>str</em>) – Column name in the <cite>matches</cite> DataFrame corresponding to the left IDs.</p></li>
<li><p><strong>matches_id_right</strong> (<em>str</em>) – Column name in the <cite>matches</cite> DataFrame corresponding to the right IDs.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A DataFrame with columns <cite>left</cite> and <cite>right</cite>, representing the indices
of matching pairs in the left and right DataFrames.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pd.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neer_match_utilities.training.Training.performance_statistics_export">
<span class="sig-name descname"><span class="pre">performance_statistics_export</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_directory</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_train</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_test</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neer_match_utilities/training.html#Training.performance_statistics_export"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neer_match_utilities.training.Training.performance_statistics_export" title="Link to this definition"></a></dt>
<dd><p>Exports the trained model, similarity map, and evaluation metrics to the specified directory.</p>
<section id="parameters">
<h3>Parameters:<a class="headerlink" href="#parameters" title="Link to this heading"></a></h3>
<dl class="simple">
<dt>model<span class="classifier">Model object</span></dt><dd><p>The trained model to export.</p>
</dd>
<dt>model_name<span class="classifier">str</span></dt><dd><p>Name of the model to use as the export directory name.</p>
</dd>
<dt>target_directory<span class="classifier">Path</span></dt><dd><p>The target directory where the model will be exported.</p>
</dd>
<dt>evaluation_train<span class="classifier">dict, optional</span></dt><dd><p>Performance metrics for the training dataset (default is {}).</p>
</dd>
<dt>evaluation_test<span class="classifier">dict, optional</span></dt><dd><p>Performance metrics for the test dataset (default is {}).</p>
</dd>
</dl>
</section>
<section id="returns">
<h3>Returns:<a class="headerlink" href="#returns" title="Link to this heading"></a></h3>
<p>:
None</p>
</section>
<section id="notes">
<h3>Notes:<a class="headerlink" href="#notes" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>The method creates a subdirectory named after <cite>model_name</cite> inside <cite>target_directory</cite>.</p></li>
<li><p>If <cite>evaluation_train</cite> and <cite>evaluation_test</cite> are provided, their metrics are saved as a CSV file.</p></li>
<li><p>Similarity maps are serialized using <cite>dill</cite> and saved in the export directory.</p></li>
</ul>
</section>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neer_match_utilities.training.TrainingPipe">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neer_match_utilities.training.</span></span><span class="sig-name descname"><span class="pre">TrainingPipe</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">testing_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">similarity_map</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_feature_width_scales=10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_depths=2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_record_width_scale=10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">record_depth=4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id_left_col='id_unique'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id_right_col='id_unique'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_tm_pbatch=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_architecture=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage_1=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage_2=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs_1=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mismatch_share_1=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage1_loss=&lt;function</span> <span class="pre">soft_f1_loss.&lt;locals&gt;.loss&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs_2=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mismatch_share_2=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_alpha=None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neer_match_utilities/training.html#TrainingPipe"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neer_match_utilities.training.TrainingPipe" title="Link to this definition"></a></dt>
<dd><p>Orchestrates the full training and evaluation process of a deep learning
record-linkage model using a user-supplied similarity map and preprocessed data.</p>
<p>The class handles both training phases (soft-F1 pretraining and focal-loss fine-tuning),
dynamic learning-rate scheduling, and automatic weight-decay adaptation. It also exports
checkpoints, final models, and evaluation statistics for reproducibility.</p>
<section id="id1">
<h2>Parameters:<a class="headerlink" href="#id1" title="Link to this heading"></a></h2>
<dl>
<dt>model_name<span class="classifier">str</span></dt><dd><p>Name assigned to the trained model. A corresponding subdirectory is created
under the project directory to store checkpoints and exports.</p>
</dd>
<dt>training_data<span class="classifier">tuple or dict</span></dt><dd><p>Preprocessed training data in one of the following formats:
- Tuple: (left_train, right_train, matches_train)
- Dict: {“left”: left_train, “right”: right_train, “matches”: matches_train}</p>
</dd>
<dt>testing_data<span class="classifier">tuple or dict</span></dt><dd><p>Preprocessed testing data in one of the following formats:
- Tuple: (left_test, right_test, matches_test)
- Dict: {“left”: left_test, “right”: right_test, “matches”: matches_test}</p>
</dd>
<dt>similarity_map<span class="classifier">dict</span></dt><dd><p>User-defined similarity configuration mapping variable names to similarity measures.
Must follow the format accepted by <cite>SimilarityMap</cite>.</p>
</dd>
<dt>id_left_col<span class="classifier">str, optional</span></dt><dd><p>Name of the unique identifier column in the left DataFrames
(<cite>left_train</cite> and <cite>left_test</cite>).
The ID column is used internally to index entities and to align
training labels. Defaults to <cite>“id_unique”</cite>.</p>
</dd>
<dt>id_right_col<span class="classifier">str, optional</span></dt><dd><p>Name of the unique identifier column in the right DataFrames
(<cite>right_train</cite> and <cite>right_test</cite>).
The ID column is used internally to index entities and to align
training labels. Defaults to <cite>“id_unique”</cite>.</p>
</dd>
<dt>no_tm_pbatch<span class="classifier">int</span></dt><dd><p>Target number of positive (matching) pairs per batch. Used to adapt batch size
dynamically via the <cite>required_batch_size</cite> heuristic.</p>
</dd>
<dt>save_architecture: bool, optional</dt><dd><p>Whether to save the model architecture in an image alongside weights when exporting.
Requires binaries of of graphviz to be installed. Otherwise, the code breaks.
Defaults to <cite>False</cite>.</p>
</dd>
<dt>stage_1<span class="classifier">bool, optional</span></dt><dd><p>Whether to run the first training stage (soft-F1 pretraining).
If False, the arguments <cite>epochs_1</cite> and <cite>mismatch_share_1</cite> are not required
and will be ignored.</p>
</dd>
<dt>stage_2<span class="classifier">bool, optional</span></dt><dd><p>Whether to run the second training stage (focal-loss fine-tuning).
If False, the arguments <cite>epochs_2</cite>, <cite>mismatch_share_2</cite>, <cite>no_tm_pbatch</cite>,
<cite>gamma</cite>, and <cite>max_alpha</cite> are not required and will be ignored.</p>
</dd>
<dt>epochs_1<span class="classifier">int, optional</span></dt><dd><p>Number of training epochs during the first phase (soft-F1 pretraining).
Required only when <cite>stage_1=True</cite>.</p>
</dd>
<dt>mismatch_share_1<span class="classifier">float, optional</span></dt><dd><p>Fraction of all possible negative (non-matching) pairs used during Round 1.
Required only when <cite>stage_1=True</cite>.</p>
</dd>
<dt>stage1_loss<span class="classifier">str or callable, optional</span></dt><dd><p>Loss function used during Stage 1 (pretraining).
By default, this is <code class="docutils literal notranslate"><span class="pre">soft_f1_loss()</span></code>, reproducing the original NeerMatch
behavior.</p>
<p>The argument accepts either:</p>
<ul>
<li><dl>
<dt>A <strong>string</strong> specifying a built-in or predefined loss:</dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;soft_f1&quot;</span></code> — use the standard soft-F1 loss (default)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;binary_crossentropy&quot;</span></code> — use wrapped binary crossentropy</p></li>
</ul>
<p>(internally adapted for NeerMatch’s evaluation loop)</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>A <strong>callable loss function</strong>, allowing full customization:</dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">soft_f1_loss()</span></code> — explicit soft-F1 loss</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">focal_loss(alpha=0.25,</span> <span class="pre">gamma=2.0)</span></code> — focal loss with parameters</p></li>
<li><p>Any user-defined loss function of signature <code class="docutils literal notranslate"><span class="pre">loss(y_true,</span> <span class="pre">y_pred)</span></code></p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>epochs_2<span class="classifier">int, optional</span></dt><dd><p>Number of training epochs during the second phase (focal-loss fine-tuning).
Required only when <cite>stage_2=True</cite>.</p>
</dd>
<dt>mismatch_share_2<span class="classifier">float, optional</span></dt><dd><p>Fraction of sampled negative pairs used during Round 2.
Required only when <cite>stage_2=True</cite>.</p>
</dd>
<dt>gamma<span class="classifier">float, optional</span></dt><dd><p>Focusing parameter of the focal loss (Round 2).
Required only when <cite>stage_2=True</cite>.</p>
</dd>
<dt>max_alpha<span class="classifier">float, optional</span></dt><dd><p>Maximum weighting factor of the positive class for focal loss (Round 2).
Required only when <cite>stage_2=True</cite>.</p>
</dd>
</dl>
</section>
<section id="id2">
<h2>Returns:<a class="headerlink" href="#id2" title="Link to this heading"></a></h2>
<p>None</p>
</section>
<section id="id3">
<h2>Notes:<a class="headerlink" href="#id3" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>The pipeline assumes that the data have already been preprocessed, formatted, and tokenized.</p></li>
<li><p>Round 1 (soft-F1 phase) initializes the model and emphasizes balanced learning across classes.</p></li>
<li><p>Round 2 (focal-loss phase) refines the model to focus on hard-to-classify examples.</p></li>
<li><dl class="simple">
<dt>Dynamic heuristics are used to automatically infer:</dt><dd><ul>
<li><p>Batch size (via expected positive density)</p></li>
<li><p>Peak learning rate (scaled with batch size, positives per batch, and parameter count)</p></li>
<li><p>Weight decay (adjusted based on model size and learning rate)</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Model checkpoints, histories, and evaluation reports are stored in subdirectories named
after the provided <cite>model_name</cite>.</p></li>
<li><p>The final model, similarity map, and performance metrics are exported to disk using the
<cite>Training.performance_statistics_export</cite> method for reproducibility.</p></li>
<li><p>Each training stage can be enabled or disabled independently through</p></li>
</ul>
<p>the <cite>stage_1</cite> and <cite>stage_2</cite> flags.
- If a stage is disabled, its hyperparameters are not required and will
be ignored.
- When only one stage is active, the warm-up pass automatically adapts
to the active stage’s mismatch sampling configuration.</p>
<dl class="py class">
<dt class="sig sig-object py" id="neer_match_utilities.training.TrainingPipe.WarmupCosine">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">WarmupCosine</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">peak_lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warmup_steps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">total_steps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_lr_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neer_match_utilities/training.html#TrainingPipe.WarmupCosine"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neer_match_utilities.training.TrainingPipe.WarmupCosine" title="Link to this definition"></a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="neer_match_utilities.training.TrainingPipe.WarmupCosine.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">peak_lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warmup_steps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">total_steps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_lr_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neer_match_utilities/training.html#TrainingPipe.WarmupCosine.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neer_match_utilities.training.TrainingPipe.WarmupCosine.__init__" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neer_match_utilities.training.TrainingPipe.WarmupCosine.from_config">
<em class="property"><span class="k"><span class="pre">classmethod</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neer_match_utilities/training.html#TrainingPipe.WarmupCosine.from_config"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neer_match_utilities.training.TrainingPipe.WarmupCosine.from_config" title="Link to this definition"></a></dt>
<dd><p>Instantiates a <cite>LearningRateSchedule</cite> from its config.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>config</strong> – Output of <cite>get_config()</cite>.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A <cite>LearningRateSchedule</cite> instance.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neer_match_utilities.training.TrainingPipe.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">testing_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">similarity_map</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_feature_width_scales=10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_depths=2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_record_width_scale=10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">record_depth=4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id_left_col='id_unique'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id_right_col='id_unique'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_tm_pbatch=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_architecture=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage_1=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage_2=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs_1=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mismatch_share_1=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage1_loss=&lt;function</span> <span class="pre">soft_f1_loss.&lt;locals&gt;.loss&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs_2=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mismatch_share_2=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_alpha=None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neer_match_utilities/training.html#TrainingPipe.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neer_match_utilities.training.TrainingPipe.__init__" title="Link to this definition"></a></dt>
<dd></dd></dl>

</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neer_match_utilities.training.alpha_balanced">
<span class="sig-prename descclassname"><span class="pre">neer_match_utilities.training.</span></span><span class="sig-name descname"><span class="pre">alpha_balanced</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">left</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">right</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">matches</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mismatch_share</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.95</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neer_match_utilities/training.html#alpha_balanced"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neer_match_utilities.training.alpha_balanced" title="Link to this definition"></a></dt>
<dd><p>Compute α so that α*N_pos = (1-α)*N_neg.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>left</strong> (<em>pandas.DataFrame</em>)</p></li>
<li><p><strong>right</strong> (<em>pandas.DataFrame</em>)</p></li>
<li><p><strong>matches</strong> (<em>pandas.DataFrame</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>α in [0,1] for focal loss (positive-class weight).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neer_match_utilities.training.combined_loss">
<span class="sig-prename descclassname"><span class="pre">neer_match_utilities.training.</span></span><span class="sig-name descname"><span class="pre">combined_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight_f1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-07</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neer_match_utilities/training.html#combined_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neer_match_utilities.training.combined_loss" title="Link to this definition"></a></dt>
<dd><p>Combined loss: weighted sum of Soft F1 loss and Focal Loss for imbalanced binary classification.</p>
<p>This loss blends the advantages of a differentiable F1-based objective (which balances
precision and recall) with the sample-focusing property of Focal Loss (which down-weights
easy examples). By tuning <code class="docutils literal notranslate"><span class="pre">weight_f1</span></code>, you can interpolate between solely optimizing
for F1 score (when <code class="docutils literal notranslate"><span class="pre">weight_f1</span> <span class="pre">=</span> <span class="pre">1.0</span></code>) and solely focusing on hard examples via focal loss
(when <code class="docutils literal notranslate"><span class="pre">weight_f1</span> <span class="pre">=</span> <span class="pre">0.0</span></code>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight_f1</strong> (<em>float</em><em>, </em><em>default=0.5</em>) – Mixing coefficient in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>.
- <code class="docutils literal notranslate"><span class="pre">weight_f1</span> <span class="pre">=</span> <span class="pre">1.0</span></code>: optimize only Soft F1 loss.
- <code class="docutils literal notranslate"><span class="pre">weight_f1</span> <span class="pre">=</span> <span class="pre">0.0</span></code>: optimize only Focal Loss.
- Intermediate values blend the two objectives proportionally.</p></li>
<li><p><strong>epsilon</strong> (<em>float</em><em>, </em><em>default=1e-7</em>) – Small stabilizer for Soft F1 calculation. Must be <code class="docutils literal notranslate"><span class="pre">&gt;</span> <span class="pre">0</span></code>.</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=0.25</em>) – Balancing factor for Focal Loss, weighting the positive (minority) class.
Must lie in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>.</p></li>
<li><p><strong>gamma</strong> (<em>float</em><em>, </em><em>default=2.0</em>) – Focusing parameter for Focal Loss.
- <code class="docutils literal notranslate"><span class="pre">gamma</span> <span class="pre">=</span> <span class="pre">0</span></code> reduces to weighted BCE.
- Larger <code class="docutils literal notranslate"><span class="pre">gamma</span></code> emphasizes harder (misclassified) examples.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>A function <code class="docutils literal notranslate"><span class="pre">loss(y_true,</span> <span class="pre">y_pred)</span></code> that computes</p>
<div class="math notranslate nohighlight">
\[\text{CombinedLoss}
= \text{weight\_f1} \cdot \text{SoftF1}(y, \hat{y};\,\varepsilon)
  + (1 - \text{weight\_f1}) \cdot \text{FocalLoss}(y, \hat{y};\,\alpha, \gamma).\]</div>
<p>Minimizing this combined loss encourages both a high F1 score
and focus on hard-to-classify samples.</p>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>callable</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If <code class="docutils literal notranslate"><span class="pre">weight_f1</span></code> is not in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>, or if <code class="docutils literal notranslate"><span class="pre">epsilon</span> <span class="pre">&lt;=</span> <span class="pre">0</span></code>, or if <code class="docutils literal notranslate"><span class="pre">alpha</span></code> is not
    in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>, or if <code class="docutils literal notranslate"><span class="pre">gamma</span> <span class="pre">&lt;</span> <span class="pre">0</span></code>.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul>
<li><p><strong>Soft F1 loss</strong>: <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">-</span> <span class="pre">\text{SoftF1}</span></code>, where</p>
<div class="math notranslate nohighlight">
\[\text{SoftF1} = \frac{2\,TP + \varepsilon}{2\,TP + FP + FN + \varepsilon}.\]</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">TP</span></code>, <code class="docutils literal notranslate"><span class="pre">FP</span></code>, and <code class="docutils literal notranslate"><span class="pre">FN</span></code> are <em>soft</em> counts computed from probabilities.</p>
</li>
<li><p><strong>Focal Loss</strong> down-weights well-classified examples to focus learning on difficult cases.</p></li>
</ul>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Lin, T.-Y., Goyal, P., Girshick, R., He, K., &amp; Dollár, P. (2017).
Focal Loss for Dense Object Detection. <em>ICCV</em>.</p></li>
<li><p>Bénédict, G., Koops, V., Odijk, D., &amp; de Rijke, M. (2021).
SigmoidF1: A Smooth F1 Score Surrogate Loss for Multilabel Classification. <em>arXiv:2108.10566</em>.</p></li>
</ul>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">combined_loss</span><span class="p">(</span><span class="n">weight_f1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>

<span class="n">y_true</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">value</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Combined loss:&quot;</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neer_match_utilities.training.focal_loss">
<span class="sig-prename descclassname"><span class="pre">neer_match_utilities.training.</span></span><span class="sig-name descname"><span class="pre">focal_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neer_match_utilities/training.html#focal_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neer_match_utilities.training.focal_loss" title="Link to this definition"></a></dt>
<dd><p>Focal Loss function for binary classification tasks.</p>
<p>Focal Loss is designed to address class imbalance by assigning higher weights
to the minority class and focusing the model’s learning on hard-to-classify examples.
It reduces the loss contribution from well-classified examples, making it
particularly effective for imbalanced datasets.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=0.75</em>) – <p>Weighting factor for the positive class (minority class).</p>
<ul>
<li><p>Must be in the range [0, 1].</p></li>
<li><p>A higher value increases the loss contribution from the positive class
(underrepresented class) relative to the negative class (overrepresented class).</p></li>
</ul>
</p></li>
<li><p><strong>gamma</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=2.0</em>) – <p>Focusing parameter that reduces the loss contribution from easy examples.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">gamma</span> <span class="pre">=</span> <span class="pre">0</span></code>: No focusing, equivalent to Weighted Binary Cross-Entropy Loss (if alpha is set to 0.5).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gamma</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>: Focuses more on hard-to-classify examples.</p></li>
<li><p>Larger values emphasize harder examples more strongly.</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>loss</strong> – A loss function that computes the focal loss given the true labels (<cite>y_true</cite>)
and predicted probabilities (<cite>y_pred</cite>).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>callable</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If <cite>alpha</cite> is not in the range [0, 1].</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>The positive class (minority or underrepresented class) is weighted by <cite>alpha</cite>.</p></li>
<li><p>The negative class (majority or overrepresented class) is automatically weighted
by <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">-</span> <span class="pre">alpha</span></code>.</p></li>
<li><p>Ensure <cite>alpha</cite> is set appropriately to reflect the level of imbalance in the dataset.</p></li>
</ul>
<p class="rubric">References</p>
<p>Lin, T.-Y., Goyal, P., Girshick, R., He, K., &amp; Dollár, P. (2017).
Focal Loss for Dense Object Detection. In ICCV.</p>
<section id="explanation-of-key-terms">
<h2>Explanation of Key Terms<a class="headerlink" href="#explanation-of-key-terms" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Positive Class (Underrepresented):</strong></p>
<ul>
<li><p>Refers to the class with fewer examples in the dataset.</p></li>
<li><p>Typically weighted by <cite>alpha</cite>, which should be greater than 0.5 in highly imbalanced datasets.</p></li>
</ul>
</li>
<li><p><strong>Negative Class (Overrepresented):</strong></p>
<ul>
<li><p>Refers to the class with more examples in the dataset.</p></li>
<li><p>Its weight is automatically <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">-</span> <span class="pre">alpha</span></code>.</p></li>
</ul>
</li>
</ul>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neer_match_utilities.training.soft_f1_loss">
<span class="sig-prename descclassname"><span class="pre">neer_match_utilities.training.</span></span><span class="sig-name descname"><span class="pre">soft_f1_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-07</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neer_match_utilities/training.html#soft_f1_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neer_match_utilities.training.soft_f1_loss" title="Link to this definition"></a></dt>
<dd><p>Soft F1 Loss for imbalanced binary classification tasks.</p>
<p>Soft F1 Loss provides a differentiable approximation of the F1 score,
combining precision and recall into a single metric. By optimizing
this loss, models are encouraged to balance false positives and false
negatives, which is especially useful when classes are imbalanced.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>epsilon</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=1e-7</em>) – Small constant added to numerator and denominator to avoid division
by zero and stabilize training. Must be &gt; 0.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>loss</strong> – A loss function that takes true labels (<cite>y_true</cite>) and predicted
probabilities (<cite>y_pred</cite>) and returns <cite>1 - soft_f1</cite>, so that
minimizing this loss maximizes the soft F1 score.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>callable</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If <cite>epsilon</cite> is not strictly positive.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>True positives (TP), false positives (FP), and false negatives (FN)
are computed in a “soft” (differentiable) manner by summing over
probabilities rather than thresholded predictions.</p></li>
<li><p>Soft F1 = (2·TP + ε) / (2·TP + FP + FN + ε).</p></li>
<li><p>Loss = 1 − Soft F1, which ranges from 0 (perfect) to 1 (worst).</p></li>
</ul>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Bénédict, G., Koops, V., Odijk D., &amp; de Rijke M. (2021). SigmoidF1: A
Smooth F1 Score Surrogate Loss for Multilabel Classification. <em>arXiv 2108.10566</em>.</p></li>
</ul>
<section id="id4">
<h2>Explanation of Key Terms<a class="headerlink" href="#id4" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>True Positives (TP):</strong> Sum of predicted probabilities for actual
positive examples.</p></li>
<li><p><strong>False Positives (FP):</strong> Sum of predicted probabilities assigned to
negative examples.</p></li>
<li><p><strong>False Negatives (FN):</strong> Sum of (1 − predicted probability) for
positive examples.</p></li>
<li><p><strong>ε (epsilon):</strong> Stabilizer to prevent division by zero when TP, FP,
and FN are all zero.</p></li>
</ul>
<p class="rubric">Examples</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">loss_fn</span> <span class="pre">=</span> <span class="pre">soft_f1_loss(epsilon=1e-6)</span>
<span class="pre">y_true</span> <span class="pre">=</span> <span class="pre">tf.constant([[1,</span> <span class="pre">0,</span> <span class="pre">1]],</span> <span class="pre">dtype=tf.float32)</span>
<span class="pre">y_pred</span> <span class="pre">=</span> <span class="pre">tf.constant([[0.9,</span> <span class="pre">0.2,</span> <span class="pre">0.7]],</span> <span class="pre">dtype=tf.float32)</span>
<span class="pre">loss_value</span> <span class="pre">=</span> <span class="pre">loss_fn(y_true,</span> <span class="pre">y_pred)</span>
<span class="pre">print(loss_value.numpy())</span>&#160; <span class="pre">#</span> <span class="pre">e.g.</span> <span class="pre">0.1…</span>
<span class="pre">`</span></code></p>
</section>
</dd></dl>

<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
</ul>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="split.html" class="btn btn-neutral float-left" title="Split" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="LICENSE.html" class="btn btn-neutral float-right" title="License" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Pantelis Karapanagiotis, Marius Liebald.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>